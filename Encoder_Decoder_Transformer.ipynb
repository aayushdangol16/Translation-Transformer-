{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj-oNU3U7kSM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset,Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding=nn.Embedding(vocab_size,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.embedding(x)*torch.sqrt(torch.tensor(self.d_model))"
      ],
      "metadata": {
        "id": "SxKspT92EgHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,seq_len=1003):\n",
        "    super().__init__()\n",
        "    self.position_embedding = nn.Embedding(seq_len+1, d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch_size, seq_len, _ = x.size()\n",
        "    positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).repeat(batch_size, 1)\n",
        "    return x + self.position_embedding(positions)"
      ],
      "metadata": {
        "id": "RscUr3I8IjfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,d_model,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(d_model,4*d_model),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*d_model,d_model),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "Qe2SbziLP7oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.layernorm=nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x,y):\n",
        "    x=self.layernorm(x+y)\n",
        "    return x"
      ],
      "metadata": {
        "id": "juAZIMC8djUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "LM35aWduQrSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderMultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,heads,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.heads=heads\n",
        "    self.d_model=d_model\n",
        "    self.d_k=d_model//heads\n",
        "\n",
        "    self.q_w=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.k_w=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.v_w=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.w_o=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query,key,value,dropout:nn.Dropout,mask=None):\n",
        "    d_k=query.shape[-1]\n",
        "    attn_scores=(query@key.transpose(-2,-1))*d_k**-0.5  # [batch,heads,seq_len,seq_len]\n",
        "    if mask is not None:  # [batch,seq_len]\n",
        "      if mask.dim()==2:\n",
        "        mask = mask.unsqueeze(1).unsqueeze(2) # [batch,1,1,seq_len]\n",
        "      attn_scores=attn_scores.masked_fill(mask==0,float('-inf'))\n",
        "    attn_scores=F.softmax(attn_scores,dim=-1)\n",
        "    if dropout is not None:\n",
        "      attn_scores=dropout(attn_scores)\n",
        "    out=attn_scores@value # [batch,heads,seq_len,d_k]\n",
        "\n",
        "    return out,attn_scores\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    query=self.q_w(x)  # [batch,seq_len,d_model]\n",
        "    key=self.k_w(x)    # [batch,seq_len,d_model]\n",
        "    value=self.v_w(x)  # [batch,seq_len,d_model]\n",
        "\n",
        "    query=query.view(query.shape[0],query.shape[1],self.heads,self.d_k).transpose(1,2)  # [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\n",
        "    key=key.view(key.shape[0],key.shape[1],self.heads,self.d_k).transpose(1,2)      # [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\n",
        "    value=value.view(value.shape[0],value.shape[1],self.heads,self.d_k).transpose(1,2)  # [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\n",
        "\n",
        "    x,self.attention_score = EncoderMultiHeadAttention.attention(query, key, value,self.dropout, mask )\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.heads * self.d_k)  # [batch,heads,seq_len, d_k] --> [batch,seq_len,head,d_k] --> [batch,seq_len,d_model]\n",
        "    x=self.dropout(self.w_o(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "TtAdSUJ8J3Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self,d_model,heads):\n",
        "    super().__init__()\n",
        "    self.SA=EncoderMultiHeadAttention(d_model,heads)\n",
        "    self.FFN=FeedForward(d_model)\n",
        "    self.ResidualConnection1=ResidualConnection(d_model)\n",
        "    self.ResidualConnection2=ResidualConnection(d_model)\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    sa_out=self.SA(x,mask)\n",
        "    x=self.ResidualConnection1(x,sa_out)\n",
        "    ffn_out=self.FFN(x)\n",
        "    x=self.ResidualConnection2(x,ffn_out)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "qAHctl_jZuKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_model,heads,layers):\n",
        "    super().__init__()\n",
        "    self.layers=layers\n",
        "    self.EncoderBlock=nn.ModuleList([EncoderBlock(d_model, heads) for _ in range(layers)])\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    for layer in self.EncoderBlock:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ijPDyY60jYJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "9JYLi-SSNEqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderMultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,heads,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.heads=heads\n",
        "    self.d_model=d_model\n",
        "    self.d_k=d_model//heads\n",
        "\n",
        "    self.q_w=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.k_w=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.v_w=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.w_o=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query,key,value,dropout:nn.Dropout,mask=None,casual=True):\n",
        "    d_k=query.shape[-1]\n",
        "    seq_len=query.shape[2]\n",
        "    attn_scores=(query@key.transpose(-2,-1))*d_k**-0.5  # [batch,heads,seq_len,seq_len]\n",
        "    if casual:\n",
        "      causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").bool()\n",
        "      causal_mask = causal_mask.unsqueeze(0).unsqueeze(1) # [1,1,seq_len,seq_len]\n",
        "\n",
        "    if mask is not None:  # [batch,seq_len]\n",
        "      if mask.dim()==2:\n",
        "        mask = mask.unsqueeze(1).unsqueeze(2) # [batch,1,1,seq_len]\n",
        "\n",
        "    combined_mask = causal_mask\n",
        "    if mask is not None:\n",
        "        combined_mask = combined_mask & mask\n",
        "\n",
        "    attn_scores=attn_scores.masked_fill(combined_mask==0,float('-inf'))\n",
        "    attn_scores=F.softmax(attn_scores,dim=-1)\n",
        "    if dropout is not None:\n",
        "      attn_scores=dropout(attn_scores)\n",
        "    out=attn_scores@value # [batch,heads,seq_len,d_k]\n",
        "\n",
        "    return out,attn_scores\n",
        "\n",
        "  def forward(self,x,mask=None,casual=True):\n",
        "    query=self.q_w(x)  # [batch,seq_len,d_model]\n",
        "    key=self.k_w(x)    # [batch,seq_len,d_model]\n",
        "    value=self.v_w(x)  # [batch,seq_len,d_model]\n",
        "\n",
        "    query=query.view(query.shape[0],query.shape[1],self.heads,self.d_k).transpose(1,2)  # [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\n",
        "    key=key.view(key.shape[0],key.shape[1],self.heads,self.d_k).transpose(1,2)      # [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\n",
        "    value=value.view(value.shape[0],value.shape[1],self.heads,self.d_k).transpose(1,2)  # [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\n",
        "\n",
        "    x,self.attention_score = DecoderMultiHeadAttention.attention(query, key, value,self.dropout, mask,casual)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.heads * self.d_k)  # [batch,heads,seq_len, d_k] --> [batch,seq_len,head,d_k] --> [batch,seq_len,d_model]\n",
        "    x=self.dropout(self.w_o(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "de_4qHU_oRN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossMultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,heads,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.heads=heads\n",
        "    self.d_model=d_model\n",
        "    self.d_k=d_model//heads\n",
        "\n",
        "    self.q_w=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.k_w=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.v_w=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.w_o=nn.Linear(d_model,d_model,bias=False)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query,key,value,dropout:nn.Dropout,mask=None):\n",
        "    d_k=query.shape[-1]\n",
        "    attn_scores=(query@key.transpose(-2,-1))*d_k**-0.5  # [batch,heads,seq_len_query,seq_len_key]\n",
        "    if mask is not None:  # [batch,seq_len_key]\n",
        "      if mask.dim()==2:\n",
        "        mask = mask.unsqueeze(1).unsqueeze(2) # [batch,1,1,seq_len]\n",
        "      attn_scores=attn_scores.masked_fill(mask==0,float('-inf'))\n",
        "    attn_scores=F.softmax(attn_scores,dim=-1)\n",
        "    if dropout is not None:\n",
        "      attn_scores=dropout(attn_scores)\n",
        "    out=attn_scores@value # [batch,heads,seq_len,d_k]\n",
        "\n",
        "    return out,attn_scores\n",
        "\n",
        "  def forward(self,decoder_output,encoder_output,mask=None):\n",
        "    query=self.q_w(decoder_output)  # [batch,seq_len,d_model]\n",
        "    key=self.k_w(encoder_output)    # [batch,seq_len,d_model]\n",
        "    value=self.v_w(encoder_output)  # [batch,seq_len,d_model]\n",
        "\n",
        "    query=query.view(query.shape[0],query.shape[1],self.heads,self.d_k).transpose(1,2)  # [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\n",
        "    key=key.view(key.shape[0],key.shape[1],self.heads,self.d_k).transpose(1,2)      # [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\n",
        "    value=value.view(value.shape[0],value.shape[1],self.heads,self.d_k).transpose(1,2)  # [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\n",
        "\n",
        "    x,self.attention_score = CrossMultiHeadAttention.attention(query, key, value,self.dropout,mask)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.heads * self.d_k)  # [batch,heads,seq_len, d_k] --> [batch,seq_len,head,d_k] --> [batch,seq_len,d_model]\n",
        "    x=self.dropout(self.w_o(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "y2rXfrxhazTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self,d_model,heads):\n",
        "    super().__init__()\n",
        "    self.MSA=DecoderMultiHeadAttention(d_model,heads)\n",
        "    self.cross_attention=CrossMultiHeadAttention(d_model,heads)\n",
        "    self.FFN=FeedForward(d_model)\n",
        "    self.ResidualConnection1=ResidualConnection(d_model) # For Self Attention\n",
        "    self.ResidualConnection2=ResidualConnection(d_model) # For Cross Attention\n",
        "    self.ResidualConnection3=ResidualConnection(d_model) # For Feed Forward\n",
        "\n",
        "  def forward(self,x,encoder_output,encode_mask=None,mask=None,casual=True):\n",
        "    # Masked Multi-Head Attention\n",
        "    msa_out=self.MSA(x,mask,casual)\n",
        "    x=self.ResidualConnection1(x,msa_out)\n",
        "    # Cross Attention\n",
        "    cross_out=self.cross_attention(x,encoder_output,encode_mask)\n",
        "    x=self.ResidualConnection2(x,cross_out)\n",
        "    # Feed Forward\n",
        "    ffn_out=self.FFN(x)\n",
        "    x=self.ResidualConnection3(x,ffn_out)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "PjGC4XYrQHmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,d_model,heads,layers):\n",
        "    super().__init__()\n",
        "    self.layers=layers\n",
        "    self.DecoderBlock=nn.ModuleList([DecoderBlock(d_model, heads) for _ in range(layers)])\n",
        "\n",
        "  def forward(self,x,encoder_output,encode_mask=None,mask=None,casual=True):\n",
        "    for layer in self.DecoderBlock:\n",
        "      x=layer(x,encoder_output,encode_mask,mask,casual)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "oQOCM2gOQM4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "e_yhNmNwx1nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model,heads,layers):\n",
        "    super().__init__()\n",
        "    self.vocab_size=vocab_size\n",
        "    self.d_model=d_model\n",
        "\n",
        "    self.inputembedding=InputEmbedding(vocab_size,d_model)\n",
        "    self.inpositionalembedding=PositionalEncoding(d_model)\n",
        "\n",
        "    self.outembedding=InputEmbedding(vocab_size,d_model)\n",
        "    self.outpositionalembedding=PositionalEncoding(d_model)\n",
        "\n",
        "    self.encoder=Encoder(d_model,heads,layers)\n",
        "    self.decoder=Decoder(d_model,heads,layers)\n",
        "\n",
        "    self.fc=nn.Linear(in_features=d_model,out_features=vocab_size)\n",
        "\n",
        "  def forward(self,encode,decode,encode_mask=None,decode_mask=None,casual=True):\n",
        "    encode=self.inputembedding(encode)\n",
        "    encode=self.inpositionalembedding(encode)\n",
        "    encode=self.encoder(encode,encode_mask)\n",
        "\n",
        "    decode=self.outembedding(decode)\n",
        "    decode=self.outpositionalembedding(decode)\n",
        "    decode=self.decoder(decode,encode,encode_mask,decode_mask,casual)\n",
        "\n",
        "    out=F.log_softmax(self.fc(decode),dim=-1)\n",
        "    return out\n",
        "\n",
        "  def encode(self):\n",
        "    encode=self.inputembedding(encode)\n",
        "    encode=self.inpositionalembedding(encode)\n",
        "    encode=self.encoder(encode,encode_mask)"
      ],
      "metadata": {
        "id": "wTPwbm42eEIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "iUg8tvOHxvCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "s3Asrl6txwkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ae4fde-273c-46c7-ab97-52d54605a651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vi6hg4Xdx5dv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa48e190-cb75-4137-89ef-c5fdc362a597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "from datasets import load_dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset,Dataset\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "SMkxOEVux_YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset(\"CohleM/english-to-nepali\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "P_a57UDxyLZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "nny9g7NSyNMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2313f71-8a1a-4eca-f4bf-2e5150134d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['en', 'ne'],\n",
              "        num_rows: 177334\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english=dataset['train']['en'][:10000]\n",
        "nepali=dataset['train']['ne'][:10000]"
      ],
      "metadata": {
        "id": "uXZqq1_iyr8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "      self.vocab={}\n",
        "\n",
        "    def load(self,path):\n",
        "      with open(path, \"rb\") as file:\n",
        "        self.vocab=pickle.load(file)\n",
        "\n",
        "    def train(self,text, vocab_size=1000):\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "        vocab_size = vocab_size\n",
        "        num_merges = vocab_size - 256\n",
        "        merges = {}\n",
        "        ids = list(tokens)\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            stats = self.get_stats(ids)\n",
        "            pair = max(stats, key=stats.get)\n",
        "            idx = 256 + i\n",
        "            ids = self.merge(ids, pair, idx)\n",
        "            merges[pair] = idx\n",
        "\n",
        "        self.vocab['vocab'] = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in merges.items():\n",
        "            self.vocab['vocab'][idx] = self.vocab['vocab'][p0] + self.vocab['vocab'][p1]\n",
        "\n",
        "        self.vocab['merges']=merges\n",
        "\n",
        "        return self.vocab\n",
        "\n",
        "    def get_stats(self, ids):\n",
        "        counts = {}\n",
        "        for pair in zip(ids, ids[1:]):\n",
        "            counts[pair] = counts.get(pair, 0) + 1\n",
        "        return counts\n",
        "\n",
        "    def merge(self, ids, pair, idx):\n",
        "        new_ids = []\n",
        "        i = 0\n",
        "        while i < len(ids):\n",
        "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "                new_ids.append(idx)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_ids.append(ids[i])\n",
        "                i += 1\n",
        "        return new_ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        token = b\"\".join(self.vocab['vocab'][idx] for idx in ids)\n",
        "        text = token.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text\n",
        "\n",
        "    def encode(self, text):\n",
        "        token = list(text.encode(\"utf-8\"))\n",
        "        while len(token) >= 2:\n",
        "            stats = self.get_stats(token)\n",
        "            pair = min(stats, key=lambda p: self.vocab['merges'].get(p, float(\"inf\")))\n",
        "            if pair not in self.vocab['merges']:\n",
        "                break\n",
        "            idx = self.vocab['merges'][pair]\n",
        "            token = self.merge(token, pair, idx)\n",
        "        return token\n"
      ],
      "metadata": {
        "id": "tlGR9V5ayvLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer=Tokenizer()\n",
        "en_tokenizer.load(\"/content/drive/MyDrive/Tokenizer/English_Tokenizer_500.pkl\")"
      ],
      "metadata": {
        "id": "i7I_gEKcy3VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_tokenizer=Tokenizer()\n",
        "np_tokenizer.load(\"/content/drive/MyDrive/Tokenizer/Nepali_Tokenizer_500.pkl\")"
      ],
      "metadata": {
        "id": "kxBUU9Ahy7VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer.vocab['vocab'][1002] = b'<sos>'\n",
        "en_tokenizer.vocab['vocab'][1001]=b'<eos>'\n",
        "en_tokenizer.vocab['vocab'][0] = b'<pad>'\n",
        "\n",
        "np_tokenizer.vocab['vocab'][1002] = b'<sos>'\n",
        "np_tokenizer.vocab['vocab'][1001]=b'<eos>'\n",
        "np_tokenizer.vocab['vocab'][0] = b'<pad>'"
      ],
      "metadata": {
        "id": "7A0xAi6tzB1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# english_tensor = [torch.tensor([1002]+en_tokenizer.encode(i) + [1001]) for i in tqdm(english)]\n",
        "# nepali_tensor = [torch.tensor([1002]+np_tokenizer.encode(i) + [1001]) for i in tqdm(nepali)]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qIqpwEa7z_qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(english_tensor),len(nepali_tensor)"
      ],
      "metadata": {
        "id": "wH3ocYrm2Mbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset=list(zip(english_tensor,nepali_tensor))"
      ],
      "metadata": {
        "id": "7cZ62zOYc_qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(dataset, '/content/drive/MyDrive/dataset/dataset.pt')"
      ],
      "metadata": {
        "id": "j8hIiNIg2u0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "lyJaLG9H_y35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=torch.load('/content/drive/MyDrive/dataset/dataset.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-gu-BXGEpAV",
        "outputId": "ac548e71-3e8d-4876-ba53-ed8df8d8d2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-78-9f0fd4c7a189>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset=torch.load('/content/drive/MyDrive/dataset/dataset.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    english_batch, nepali_batch = zip(*batch)\n",
        "\n",
        "    max_len_english = max(len(seq) for seq in english_batch)\n",
        "    max_len_nepali = max(len(seq) for seq in nepali_batch)\n",
        "\n",
        "    padded_english_batch = pad_sequence(english_batch, batch_first=True, padding_value=0)\n",
        "    padded_nepali_batch = pad_sequence(nepali_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    padded_english_batch = F.pad(padded_english_batch, (0, max_len_english - padded_english_batch.size(1)))\n",
        "    padded_nepali_batch = F.pad(padded_nepali_batch, (0, max_len_nepali - padded_nepali_batch.size(1)))\n",
        "\n",
        "    x_padding_mask = (padded_english_batch != 1000).int()\n",
        "    y_padding_mask = (padded_nepali_batch != 1000).int()\n",
        "\n",
        "    return padded_english_batch,x_padding_mask, padded_nepali_batch,y_padding_mask\n"
      ],
      "metadata": {
        "id": "wd0FSzBNbJYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, drop_last=True)"
      ],
      "metadata": {
        "id": "kiuJEiYj24Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_train_mask,y_train,y_train_mask=next(iter(dataloader))"
      ],
      "metadata": {
        "id": "I4pLCfbH3NeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape,x_train_mask.shape,y_train.shape,y_train_mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSHTzXKY3ZQv",
        "outputId": "20c34a95-be40-41fc-9436-755136b3f33b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 144]),\n",
              " torch.Size([32, 144]),\n",
              " torch.Size([32, 143]),\n",
              " torch.Size([32, 143]))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "MJNae1oVFISQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer(vocab_size=1003,d_model=168,heads=8,layers=2)"
      ],
      "metadata": {
        "id": "hDzqRtSi3biU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model.to(device)\n",
        "x_train,x_train_mask,y_train,y_train_mask=x_train.to(device),x_train_mask.to(device),y_train.to(device),y_train_mask.to(device)"
      ],
      "metadata": {
        "id": "NWMGNqrJFU_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out=model(x_train,y_train,x_train_mask,y_train_mask)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mn0d2lju8jmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jseda01DLcfD",
        "outputId": "2e01f72e-245d-4bba-b7fc-041d39e3fa53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 143, 1003])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer(vocab_size=1003,d_model=300,heads=8,layers=6).to(device)\n",
        "optimizer=optim.AdamW(model.parameters(), lr=0.001,weight_decay=0.01)\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "5JQ5ERSKe9hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss=[]\n",
        "\n",
        "for i in range(50):\n",
        "  model.train()\n",
        "  batch_train_loss=[]\n",
        "  for batch in tqdm(dataloader,leave=False):\n",
        "    x_train=batch[0].to(device)\n",
        "    x_train_mask=batch[1].to(device)\n",
        "    y_train=batch[2].to(device)\n",
        "    y_train_mask=batch[3].to(device)\n",
        "\n",
        "\n",
        "    output=model(x_train,y_train,x_train_mask,y_train_mask)\n",
        "    loss = criterion(output.view(-1,output.size(-1)),y_train.view(-1))\n",
        "    batch_train_loss.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss.append(sum(batch_train_loss)/len(batch_train_loss))\n",
        "  print(f\"Epoch={i}\\tTrain Loss={sum(batch_train_loss)/len(batch_train_loss)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "collapsed": true,
        "id": "7fVOIAJ3fnC_",
        "outputId": "eda9f31c-5aae-4842-ff50-6f148222083b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 21.06 MiB is free. Process 7391 has 14.72 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 713.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-0743416cfdc3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbatch_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-4cb9f737b5bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encode, decode, encode_mask, decode_mask, casual)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdecode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutembedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdecode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutpositionalembedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdecode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencode_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecode_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcasual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-2e7675b38785>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_output, encode_mask, mask, casual)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencode_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcasual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecoderBlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencode_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcasual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-68f1811e2a49>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_output, encode_mask, mask, casual)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResidualConnection1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmsa_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Cross Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcross_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencode_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResidualConnection2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Feed Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-3dadbb75ee3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_output, encoder_output, mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch,seq_len,d_model]=>[batch,heads,seq_len,d_k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossMultiHeadAttention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch,heads,seq_len, d_k] --> [batch,seq_len,head,d_k] --> [batch,seq_len,d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-3dadbb75ee3e>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(query, key, value, dropout, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0md_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mattn_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m  \u001b[0;31m# [batch,heads,seq_len_query,seq_len_key]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# [batch,seq_len_key]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 21.06 MiB is free. Process 7391 has 14.72 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 713.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "USlfzuMVqQjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss,label=\"Train Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gPAcPNXmh6OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_text = english[0]\n",
        "target = nepali[0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    input_tensor = torch.tensor([1002] + en_tokenizer.encode(eng_text) + [1001]).unsqueeze(0).to(device)\n",
        "    decoder_input = torch.tensor([[1002]], device=device)  # Shape: [1, 1]\n",
        "\n",
        "    predicted_tokens = []\n",
        "\n",
        "    for _ in range(10):\n",
        "        output = model(input_tensor, decoder_input)\n",
        "        output = output[:, -1, :]\n",
        "\n",
        "        probs = torch.exp(output)\n",
        "        probs[:, 0] = 0\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        predicted_tokens.append(next_token.squeeze().item())\n",
        "\n",
        "        if next_token.item() == 1001:\n",
        "            break\n",
        "\n",
        "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
        "\n",
        "    predicted_text = np_tokenizer.decode(predicted_tokens)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Input: {eng_text}\")\n",
        "print(f\"Target: {target}\")\n",
        "print(f\"Predicted: {predicted_text}\")\n"
      ],
      "metadata": {
        "id": "26e8algwiQ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2pMaB8jq7zu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}